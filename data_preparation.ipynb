{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Preparation for PD vs Healthy Control EEG\n",
        "#\n",
        "# This notebook:\n",
        "# 1. Loads raw EEG (.vhdr) files from a mixed PD/Control folder\n",
        "# 2. Preprocesses EEG and extracts features\n",
        "# 3. Cleans the data (missing values, duplicates, outliers)\n",
        "# 4. Runs basic EDA\n",
        "# 5. Transforms data (scaling) and performs feature selection\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Display plots inline\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "\n",
        "# Path to your mixed PD/Control EEG folder (with .vhdr/.vmrk/.eeg per subject)\n",
        "DATA_DIR = r\"C:\\Users\\Usha Sri\\OneDrive\\Documents\\Parkinson_Project\\JamesCavanagh\\PD_Dataset_timing\"  # change if needed\n",
        "\n",
        "# EEG preprocessing params\n",
        "L_FREQ = 1.0       # high-pass\n",
        "H_FREQ = 40.0      # low-pass\n",
        "NOTCH_FREQ = 50.0  # or 60.0 depending on mains\n",
        "RESAMPLE = 128     # Hz, or None to keep original\n",
        "EPOCH_LENGTH = 2.0  # seconds\n",
        "\n",
        "# Outlier handling\n",
        "Z_THRESHOLD = 4.0  # for z-score based outlier removal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers: labels and subject IDs\n",
        "\n",
        "def get_label_from_filename(fname):\n",
        "    \"\"\"Return 1 for PD, 0 for Control (handles 'Contorl' typo).\"\"\"\n",
        "    base = os.path.basename(fname).lower()\n",
        "    if base.startswith(\"pd\"):\n",
        "        return 1\n",
        "    if base.startswith(\"control\") or base.startswith(\"contorl\"):\n",
        "        return 0\n",
        "    raise ValueError(f\"Cannot determine label from filename: {fname}\")\n",
        "\n",
        "\n",
        "def get_subject_id_from_filename(fname):\n",
        "    \"\"\"Example: PD1325.vhdr -> PD1325\"\"\"\n",
        "    base = os.path.basename(fname)\n",
        "    subj_id = os.path.splitext(base)[0]\n",
        "    return subj_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EEG preprocessing and feature extraction\n",
        "\n",
        "def preprocess_raw(vhdr_path,\n",
        "                   l_freq=L_FREQ,\n",
        "                   h_freq=H_FREQ,\n",
        "                   notch_freq=NOTCH_FREQ,\n",
        "                   resample=RESAMPLE):\n",
        "    print(f\"Loading {vhdr_path}\")\n",
        "    raw = mne.io.read_raw_brainvision(vhdr_path, preload=True)\n",
        "\n",
        "    # Set average reference\n",
        "    raw.set_eeg_reference(\"average\", projection=False)\n",
        "\n",
        "    # Band-pass filter\n",
        "    raw.filter(l_freq=l_freq, h_freq=h_freq)\n",
        "\n",
        "    # Notch filter\n",
        "    if notch_freq is not None:\n",
        "        raw.notch_filter(freqs=[notch_freq])\n",
        "\n",
        "    # Resample\n",
        "    if resample is not None and raw.info[\"sfreq\"] != resample:\n",
        "        raw.resample(resample)\n",
        "\n",
        "    return raw\n",
        "\n",
        "\n",
        "def make_fixed_length_epochs(raw, epoch_length=EPOCH_LENGTH):\n",
        "    epochs = mne.make_fixed_length_epochs(\n",
        "        raw,\n",
        "        duration=epoch_length,\n",
        "        preload=True\n",
        "    )\n",
        "    return epochs\n",
        "\n",
        "\n",
        "def extract_features_from_epochs(epochs):\n",
        "    \"\"\"Flatten each epoch + add mean and std per channel as extra features.\"\"\"\n",
        "    data = epochs.get_data()  # (n_epochs, n_channels, n_times)\n",
        "    n_epochs, n_channels, n_times = data.shape\n",
        "\n",
        "    flat = data.reshape(n_epochs, n_channels * n_times)\n",
        "    mean_feats = data.mean(axis=2)  # (n_epochs, n_channels)\n",
        "    std_feats = data.std(axis=2)   # (n_epochs, n_channels)\n",
        "\n",
        "    features = np.concatenate([flat, mean_feats, std_feats], axis=1)\n",
        "    return features\n",
        "\n",
        "\n",
        "def process_subject(vhdr_path):\n",
        "    \"\"\"Process one subject/file into a feature DataFrame with labels and meta.\"\"\"\n",
        "    label = get_label_from_filename(vhdr_path)\n",
        "    subject_id = get_subject_id_from_filename(vhdr_path)\n",
        "\n",
        "    raw = preprocess_raw(vhdr_path)\n",
        "    epochs = make_fixed_length_epochs(raw)\n",
        "    X_sub = extract_features_from_epochs(epochs)\n",
        "\n",
        "    n_epochs, n_features = X_sub.shape\n",
        "    feature_cols = [f\"feat_{i}\" for i in range(n_features)]\n",
        "\n",
        "    df_sub = pd.DataFrame(X_sub, columns=feature_cols)\n",
        "    df_sub[\"label\"] = label\n",
        "    df_sub[\"subject_id\"] = subject_id\n",
        "    df_sub[\"epoch_idx\"] = np.arange(n_epochs)\n",
        "\n",
        "    return df_sub\n",
        "\n",
        "\n",
        "def build_raw_feature_dataset(data_dir=DATA_DIR):\n",
        "    \"\"\"Build raw feature DataFrame from all .vhdr files in folder.\"\"\"\n",
        "    vhdr_files = sorted(glob.glob(os.path.join(data_dir, \"*.vhdr\")))\n",
        "    if not vhdr_files:\n",
        "        raise FileNotFoundError(f\"No .vhdr files found in {data_dir}\")\n",
        "\n",
        "    all_dfs = []\n",
        "    for f in vhdr_files:\n",
        "        try:\n",
        "            df_sub = process_subject(f)\n",
        "            all_dfs.append(df_sub)\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping file: {e}\")\n",
        "\n",
        "    full_df = pd.concat(all_dfs, axis=0, ignore_index=True)\n",
        "    print(\"Raw feature DataFrame shape:\", full_df.shape)\n",
        "    return full_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data cleaning: duplicates, missing values, outliers\n",
        "\n",
        "def clean_data(df):\n",
        "    print(\"\\n--- Data Cleaning ---\")\n",
        "\n",
        "    meta_cols = [\"label\", \"subject_id\", \"epoch_idx\"]\n",
        "    feature_cols = [c for c in df.columns if c not in meta_cols]\n",
        "\n",
        "    # 1) Remove duplicates\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates()\n",
        "    after = len(df)\n",
        "    print(f\"Removed duplicates: {before - after}\")\n",
        "\n",
        "    # 2) Handle missing values\n",
        "    #    a) Drop features with > 40% missing\n",
        "    missing_ratio = df[feature_cols].isna().mean()\n",
        "    keep_features = missing_ratio[missing_ratio <= 0.4].index.tolist()\n",
        "    dropped_features = [f for f in feature_cols if f not in keep_features]\n",
        "    if dropped_features:\n",
        "        print(f\"Dropping {len(dropped_features)} features with >40% missing values\")\n",
        "    df = df[keep_features + meta_cols]\n",
        "\n",
        "    feature_cols = keep_features\n",
        "\n",
        "    #    b) Impute remaining missing values with median\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    df[feature_cols] = imputer.fit_transform(df[feature_cols])\n",
        "\n",
        "    # 3) Outlier handling via z-score\n",
        "    feature_data = df[feature_cols].values.astype(float)\n",
        "    mean = feature_data.mean(axis=0)\n",
        "    std = feature_data.std(axis=0)\n",
        "    std[std == 0] = 1.0\n",
        "\n",
        "    z_scores = (feature_data - mean) / std\n",
        "    max_abs_z = np.max(np.abs(z_scores), axis=1)\n",
        "    mask = max_abs_z <= Z_THRESHOLD\n",
        "\n",
        "    removed_outliers = np.sum(~mask)\n",
        "    print(f\"Removed outlier rows: {removed_outliers}\")\n",
        "\n",
        "    df_clean = df.loc[mask].reset_index(drop=True)\n",
        "\n",
        "    print(\"Cleaned DataFrame shape:\", df_clean.shape)\n",
        "    return df_clean, feature_cols\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "def run_basic_eda(df, feature_cols, save_plots=False, out_dir=\"eda_plots\"):\n",
        "    print(\"\\n--- Exploratory Data Analysis (EDA) ---\")\n",
        "\n",
        "    if save_plots and not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Class balance\n",
        "    print(\"Class counts (0=Control, 1=PD):\")\n",
        "    print(df[\"label\"].value_counts())\n",
        "\n",
        "    # Histograms for a few features\n",
        "    sample_features = feature_cols[:5]\n",
        "    for col in sample_features:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.histplot(data=df, x=col, hue=\"label\", kde=True,\n",
        "                     stat=\"density\", common_norm=False)\n",
        "        plt.title(f\"Distribution of {col} by class\")\n",
        "        if save_plots:\n",
        "            plt.savefig(os.path.join(out_dir, f\"hist_{col}.png\"), dpi=150)\n",
        "        plt.show()\n",
        "\n",
        "    # Correlation heatmap for a subset of features\n",
        "    corr_features = feature_cols[:30]\n",
        "    corr = df[corr_features].corr()\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
        "    plt.title(\"Correlation heatmap (subset of features)\")\n",
        "    if save_plots:\n",
        "        plt.savefig(os.path.join(out_dir, \"corr_heatmap.png\"), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"EDA plots generated (and saved if save_plots=True).\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transformation and feature selection\n",
        "\n",
        "def transform_and_select_features(df, feature_cols):\n",
        "    \"\"\"Standardize, remove low-variance features, and select informative ones.\"\"\"\n",
        "    print(\"\\n--- Data Transformation & Feature Selection ---\")\n",
        "\n",
        "    X = df[feature_cols].values\n",
        "    y = df[\"label\"].values\n",
        "\n",
        "    # 1) Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # 2) Remove near-constant features\n",
        "    vt = VarianceThreshold(threshold=1e-5)\n",
        "    X_vt = vt.fit_transform(X_scaled)\n",
        "    kept_mask_vt = vt.get_support()\n",
        "    kept_features_vt = [f for f, keep in zip(feature_cols, kept_mask_vt) if keep]\n",
        "    print(f\"Features after VarianceThreshold: {len(kept_features_vt)}\")\n",
        "\n",
        "    # 3) Model-based selection\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf.fit(X_vt, y)\n",
        "    selector = SelectFromModel(rf, prefit=True, threshold=\"median\")\n",
        "    X_final = selector.transform(X_vt)\n",
        "    kept_mask_model = selector.get_support()\n",
        "    selected_features = [\n",
        "        f for f, keep in zip(kept_features_vt, kept_mask_model) if keep\n",
        "    ]\n",
        "\n",
        "    print(f\"Features after model-based selection: {len(selected_features)}\")\n",
        "\n",
        "    return X_final, y, selected_features, scaler, vt, selector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the full data preparation pipeline\n",
        "\n",
        "# 1) Build raw feature dataset from EEG\n",
        "df_raw = build_raw_feature_dataset(DATA_DIR)\n",
        "\n",
        "# 2) Data cleaning\n",
        "df_clean, feature_cols = clean_data(df_raw)\n",
        "\n",
        "# 3) EDA (set save_plots=True to also save PNGs to disk)\n",
        "run_basic_eda(df_clean, feature_cols, save_plots=False)\n",
        "\n",
        "# 4) Transformation + feature selection\n",
        "X_final, y, selected_features, scaler, vt, selector = transform_and_select_features(\n",
        "    df_clean, feature_cols\n",
        ")\n",
        "\n",
        "print(\"\\nFinal data ready for modeling:\")\n",
        "print(\"X_final shape:\", X_final.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# Optionally save cleaned dataset and selected feature list\n",
        "df_clean.to_csv(\"cleaned_eeg_features.csv\", index=False)\n",
        "pd.Series(selected_features).to_csv(\"selected_features.txt\", index=False)\n",
        "print(\"Saved cleaned data and selected feature list.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next step: Model training\n",
        "\n",
        "After running this notebook, you can use `X_final` and `y` to train your\n",
        "classification model (e.g., RandomForest, SVM, deep learning). You can either:\n",
        "\n",
        "- Add new cells **below** to train a classifier directly in this notebook, or\n",
        "- Save `X_final` and `y` to disk and load them in a separate modeling notebook.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
