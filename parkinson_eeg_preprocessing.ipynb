{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parkinson's Disease EEG Data Preprocessing Pipeline\n",
        "\n",
        "This notebook implements a **memory-efficient** preprocessing pipeline for **combining two EEG datasets**:\n",
        "1. **PD_Dataset_timing**: BrainVision format (.vhdr, .eeg, .vmrk) - 129 subjects\n",
        "2. **ds004584-download**: BIDS format with EEGLAB files (.set, .fdt) - 149 subjects\n",
        "\n",
        "## Important: Combined Dataset Processing\n",
        "**Both datasets are processed together** in the same pipeline with **uniform preprocessing steps**. This ensures:\n",
        "- Consistent preprocessing across all subjects regardless of source dataset\n",
        "- Same parameters applied to both datasets (resampling, filtering, re-referencing, etc.)\n",
        "- Combined output ready for cross-dataset analysis\n",
        "- Dataset source is tracked in metadata but doesn't affect processing\n",
        "\n",
        "## Pipeline Steps:\n",
        "1. Data Loading from both datasets (one subject at a time)\n",
        "2. Standardized Signal Pre-processing (Resampling, Filtering, Re-referencing)\n",
        "3. Artifact Detection & Correction (ICA without ICLabel, Bad Channel Interpolation)\n",
        "4. Subject-Level Alignment (Z-Score Normalization, Riemannian Re-centering)\n",
        "5. Save preprocessed data for each subject\n",
        "\n",
        "## Memory-Efficient Design:\n",
        "- Processes one subject at a time to avoid memory issues\n",
        "- Saves each preprocessed subject immediately\n",
        "- Can resume from saved checkpoints\n",
        "- No ICLabel dependency (uses correlation-based artifact detection)\n",
        "- Robust error handling for NaN/Inf values and numerical issues\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Required Packages Installation\n",
        "\n",
        "Before running this notebook, install the required packages:\n",
        "\n",
        "```bash\n",
        "pip install mne mne-bids pyriemann scipy numpy pandas\n",
        "```\n",
        "\n",
        "**Note**: This notebook does NOT require ICLabel or GPU. It uses memory-efficient, CPU-based processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "MNE version: 1.11.0\n",
            "NumPy version: 2.0.2\n",
            "PyRiemann available: True\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "from mne.io import read_raw_brainvision, read_raw_eeglab\n",
        "from mne.preprocessing import ICA\n",
        "from scipy import stats\n",
        "from scipy.signal import find_peaks\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set MNE logging level to reduce output\n",
        "mne.set_log_level('ERROR')\n",
        "\n",
        "# Try to import pyriemann for Riemannian re-centering (optional)\n",
        "try:\n",
        "    from pyriemann.utils.mean import mean_riemann\n",
        "    HAS_PYRIEMANN = True\n",
        "except ImportError:\n",
        "    HAS_PYRIEMANN = False\n",
        "    print(\"Warning: pyriemann not available. Riemannian re-centering will be skipped.\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"MNE version: {mne.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"PyRiemann available: {HAS_PYRIEMANN}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration and Paths\n",
        "\n",
        "Define paths, preprocessing parameters, and output directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PD Dataset Path: C:\\Users\\Usha Sri\\OneDrive\\Documents\\Parkinson_Project\\PD_Dataset_timing\n",
            "BIDS Dataset Path: C:\\Users\\Usha Sri\\OneDrive\\Documents\\Parkinson_Project\\ds004584-download\n",
            "Output Directory: C:\\Users\\Usha Sri\\OneDrive\\Documents\\Parkinson_Project\\preprocessed_data\n",
            "✓ Both dataset paths verified!\n",
            "\n",
            "Preprocessing Parameters:\n",
            "  Target Sampling Rate: 250 Hz\n",
            "  Band-pass Filter: 0.5 - 50 Hz\n",
            "  ICA Components: 15\n"
          ]
        }
      ],
      "source": [
        "# Define dataset paths\n",
        "BASE_PATH = r\"C:\\Users\\Usha Sri\\OneDrive\\Documents\\Parkinson_Project\"\n",
        "PD_DATASET_PATH = os.path.join(BASE_PATH, \"PD_Dataset_timing\")\n",
        "BIDS_DATASET_PATH = os.path.join(BASE_PATH, \"ds004584-download\")\n",
        "OUTPUT_DIR = os.path.join(BASE_PATH, \"preprocessed_data\")\n",
        "\n",
        "# Preprocessing parameters\n",
        "TARGET_SFREQ = 250  # Target sampling rate (Hz)\n",
        "LOW_FREQ = 0.5      # Low cutoff frequency (Hz)\n",
        "HIGH_FREQ = 50      # High cutoff frequency (Hz)\n",
        "\n",
        "# ICA parameters (reduced for memory efficiency)\n",
        "ICA_N_COMPONENTS = 15  # Reduced from 20 for memory efficiency\n",
        "ICA_MAX_ITER = 200     # Reduced iterations for speed\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"PD Dataset Path: {PD_DATASET_PATH}\")\n",
        "print(f\"BIDS Dataset Path: {BIDS_DATASET_PATH}\")\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Verify paths exist\n",
        "assert os.path.exists(PD_DATASET_PATH), f\"PD dataset path not found: {PD_DATASET_PATH}\"\n",
        "assert os.path.exists(BIDS_DATASET_PATH), f\"BIDS dataset path not found: {BIDS_DATASET_PATH}\"\n",
        "print(\"✓ Both dataset paths verified!\")\n",
        "print(f\"\\nPreprocessing Parameters:\")\n",
        "print(f\"  Target Sampling Rate: {TARGET_SFREQ} Hz\")\n",
        "print(f\"  Band-pass Filter: {LOW_FREQ} - {HIGH_FREQ} Hz\")\n",
        "print(f\"  ICA Components: {ICA_N_COMPONENTS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning PD_Dataset_timing (BrainVision format)...\n",
            "Scanning ds004584-download (BIDS format)...\n",
            "\n",
            "✓ Found 129 files in PD_Dataset_timing\n",
            "✓ Found 149 files in ds004584-download\n",
            "✓ Total: 278 files to process\n"
          ]
        }
      ],
      "source": [
        "# Function to get file list (without loading data)\n",
        "def get_brainvision_file_list(dataset_path):\n",
        "    \"\"\"Get list of BrainVision files without loading them.\"\"\"\n",
        "    vhdr_files = [f for f in os.listdir(dataset_path) if f.endswith('.vhdr')]\n",
        "    vhdr_files.sort()\n",
        "    \n",
        "    file_list = []\n",
        "    for vhdr_file in vhdr_files:\n",
        "        file_path = os.path.join(dataset_path, vhdr_file)\n",
        "        subject_id = vhdr_file.replace('.vhdr', '')\n",
        "        group = 'Control' if 'control' in subject_id.lower() else 'PD'\n",
        "        file_list.append({\n",
        "            'subject_id': subject_id,\n",
        "            'group': group,\n",
        "            'file_path': file_path,\n",
        "            'dataset': 'PD_Dataset_timing'\n",
        "        })\n",
        "    \n",
        "    return file_list\n",
        "\n",
        "def get_bids_file_list(dataset_path):\n",
        "    \"\"\"Get list of BIDS files without loading them.\"\"\"\n",
        "    subject_dirs = [d for d in os.listdir(dataset_path) if d.startswith('sub-')]\n",
        "    subject_dirs.sort()\n",
        "    \n",
        "    file_list = []\n",
        "    for subject_dir in subject_dirs:\n",
        "        eeg_path = os.path.join(dataset_path, subject_dir, 'eeg')\n",
        "        if not os.path.exists(eeg_path):\n",
        "            continue\n",
        "        \n",
        "        set_files = [f for f in os.listdir(eeg_path) if f.endswith('.set')]\n",
        "        if not set_files:\n",
        "            continue\n",
        "        \n",
        "        set_file = os.path.join(eeg_path, set_files[0])\n",
        "        file_list.append({\n",
        "            'subject_id': subject_dir,\n",
        "            'group': 'PD',\n",
        "            'file_path': set_file,\n",
        "            'dataset': 'ds004584-download'\n",
        "        })\n",
        "    \n",
        "    return file_list\n",
        "\n",
        "# Get file lists (memory efficient - no data loaded yet)\n",
        "print(\"Scanning PD_Dataset_timing (BrainVision format)...\")\n",
        "pd_file_list = get_brainvision_file_list(PD_DATASET_PATH)\n",
        "\n",
        "print(\"Scanning ds004584-download (BIDS format)...\")\n",
        "bids_file_list = get_bids_file_list(BIDS_DATASET_PATH)\n",
        "\n",
        "# Combine file lists\n",
        "all_file_list = pd_file_list + bids_file_list\n",
        "\n",
        "print(f\"\\n✓ Found {len(pd_file_list)} files in PD_Dataset_timing\")\n",
        "print(f\"✓ Found {len(bids_file_list)} files in ds004584-download\")\n",
        "print(f\"✓ Total: {len(all_file_list)} files to process\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing Functions\n",
        "\n",
        "Define memory-efficient preprocessing functions that process one subject at a time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Preprocessing functions defined\n"
          ]
        }
      ],
      "source": [
        "def load_single_file(file_info):\n",
        "    \"\"\"Load a single file based on its format.\"\"\"\n",
        "    file_path = file_info['file_path']\n",
        "    \n",
        "    if file_info['dataset'] == 'PD_Dataset_timing':\n",
        "        # BrainVision format\n",
        "        raw = read_raw_brainvision(file_path, preload=False)\n",
        "    else:\n",
        "        # EEGLAB format\n",
        "        raw = read_raw_eeglab(file_path, preload=False)\n",
        "    \n",
        "    return raw\n",
        "\n",
        "def detect_bad_channels_simple(raw):\n",
        "    \"\"\"\n",
        "    Simple bad channel detection using variance and flat signal detection.\n",
        "    Memory-efficient alternative to find_bad_channels.\n",
        "    Very conservative approach to avoid false positives.\n",
        "    \"\"\"\n",
        "    raw_copy = raw.copy()\n",
        "    raw_copy.load_data()\n",
        "    \n",
        "    bads = []\n",
        "    data = raw_copy.get_data()\n",
        "    \n",
        "    # Check for infs and NaNs first - these are definitely bad\n",
        "    for i, ch_name in enumerate(raw_copy.ch_names):\n",
        "        ch_data = data[i, :]\n",
        "        if np.any(np.isnan(ch_data)) or np.any(np.isinf(ch_data)):\n",
        "            bads.append(ch_name)\n",
        "    \n",
        "    # Detect flat channels (very low variance) - very conservative threshold\n",
        "    channel_vars = np.var(data, axis=1)\n",
        "    # Use a very strict absolute threshold - only mark channels that are truly flat\n",
        "    flat_threshold = 1e-8  # Very strict threshold\n",
        "    \n",
        "    for i, ch_var in enumerate(channel_vars):\n",
        "        if ch_var < flat_threshold and raw_copy.ch_names[i] not in bads:\n",
        "            bads.append(raw_copy.ch_names[i])\n",
        "    \n",
        "    # Detect channels with extreme values (likely disconnected) - very conservative\n",
        "    channel_max = np.max(np.abs(data), axis=1)\n",
        "    channel_median = np.median(channel_max)\n",
        "    \n",
        "    # Only mark channels that are EXTREMELY different (20x median or more)\n",
        "    # This is much more conservative than before\n",
        "    extreme_threshold = channel_median * 20\n",
        "    \n",
        "    for i, ch_max in enumerate(channel_max):\n",
        "        if ch_max > extreme_threshold and raw_copy.ch_names[i] not in bads:\n",
        "            bads.append(raw_copy.ch_names[i])\n",
        "    \n",
        "    # Very strict limit: max 5% of channels (or at least 1 if we have very few channels)\n",
        "    max_bad_channels = max(1, min(5, int(len(raw_copy.ch_names) * 0.05)))\n",
        "    if len(bads) > max_bad_channels:\n",
        "        # If too many, only keep the most extreme ones based on variance\n",
        "        bad_scores = []\n",
        "        for ch_name in bads:\n",
        "            ch_idx = raw_copy.ch_names.index(ch_name)\n",
        "            # Score based on how bad the channel is (lower variance = worse)\n",
        "            var_score = 1.0 / (channel_vars[ch_idx] + 1e-12)\n",
        "            bad_scores.append((ch_name, var_score))\n",
        "        \n",
        "        bad_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        bads = [ch for ch, _ in bad_scores[:max_bad_channels]]\n",
        "    \n",
        "    return bads\n",
        "\n",
        "def preprocess_raw(raw, subject_id):\n",
        "    \"\"\"\n",
        "    Apply standardized preprocessing to a Raw object.\n",
        "    Memory-efficient version.\n",
        "    \"\"\"\n",
        "    # Load data into memory\n",
        "    raw.load_data()\n",
        "    \n",
        "    # Step 1: Resampling to target sampling rate\n",
        "    if raw.info['sfreq'] != TARGET_SFREQ:\n",
        "        raw.resample(TARGET_SFREQ, npad='auto')\n",
        "    \n",
        "    # Step 2: Band-pass filtering (0.5 - 50 Hz)\n",
        "    raw.filter(LOW_FREQ, HIGH_FREQ, fir_design='firwin', \n",
        "               skip_by_annotation='edge', verbose=False)\n",
        "    \n",
        "    # Step 3: Common Average Reference (CAR)\n",
        "    raw.set_eeg_reference('average', projection=False, verbose=False)\n",
        "    \n",
        "    return raw\n",
        "\n",
        "def detect_artifacts_ica_simple(raw, subject_id):\n",
        "    \"\"\"\n",
        "    Simple ICA-based artifact detection without ICLabel.\n",
        "    Uses correlation-based detection for EOG/ECG artifacts.\n",
        "    Includes data cleaning to handle NaN/Inf values.\n",
        "    \"\"\"\n",
        "    # Make a copy for ICA\n",
        "    raw_ica = raw.copy()\n",
        "    \n",
        "    # Check for and clean NaN/Inf values before ICA\n",
        "    data = raw_ica.get_data()\n",
        "    if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
        "        print(f\"    Warning: Found NaN/Inf values, cleaning data...\")\n",
        "        # Replace NaN/Inf with zeros (or median of channel)\n",
        "        for i in range(data.shape[0]):\n",
        "            ch_data = data[i, :]\n",
        "            if np.any(np.isnan(ch_data)) or np.any(np.isinf(ch_data)):\n",
        "                # Replace with median of valid values\n",
        "                valid_data = ch_data[np.isfinite(ch_data)]\n",
        "                if len(valid_data) > 0:\n",
        "                    replacement = np.median(valid_data)\n",
        "                else:\n",
        "                    replacement = 0.0\n",
        "                ch_data[np.isnan(ch_data) | np.isinf(ch_data)] = replacement\n",
        "                data[i, :] = ch_data\n",
        "        raw_ica._data = data\n",
        "    \n",
        "    # High-pass filter at 1 Hz for ICA (recommended)\n",
        "    raw_ica.filter(1., None, fir_design='firwin', \n",
        "                   skip_by_annotation='edge', verbose=False)\n",
        "    \n",
        "    # Check again after filtering\n",
        "    data = raw_ica.get_data()\n",
        "    if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
        "        print(f\"    Warning: NaN/Inf values after filtering, skipping ICA...\")\n",
        "        return raw, []\n",
        "    \n",
        "    # Determine number of components (use fewer for memory efficiency)\n",
        "    n_channels = len(raw_ica.ch_names)\n",
        "    n_components = min(ICA_N_COMPONENTS, n_channels - 1)\n",
        "    \n",
        "    if n_components < 2:\n",
        "        return raw, []  # Not enough channels for ICA\n",
        "    \n",
        "    # Fit ICA with reduced iterations\n",
        "    ica = ICA(n_components=n_components, random_state=42, \n",
        "              max_iter=ICA_MAX_ITER, verbose=False)\n",
        "    \n",
        "    try:\n",
        "        # Fit ICA with decimation for speed\n",
        "        ica.fit(raw_ica, decim=5, verbose=False)\n",
        "        \n",
        "        # Find artifacts using correlation with EOG/ECG patterns\n",
        "        exclude_idx = []\n",
        "        \n",
        "        # Try to find EOG artifacts (if EOG channels exist)\n",
        "        eog_channels = [ch for ch in raw_ica.ch_names if 'EOG' in ch.upper() or 'Fp' in ch]\n",
        "        if len(eog_channels) > 0:\n",
        "            try:\n",
        "                eog_indices, eog_scores = ica.find_bads_eog(raw_ica, \n",
        "                                                             ch_name=eog_channels[0],\n",
        "                                                             verbose=False)\n",
        "                exclude_idx.extend(eog_indices)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        # Try to find ECG artifacts\n",
        "        try:\n",
        "            ecg_indices, ecg_scores = ica.find_bads_ecg(raw_ica, verbose=False)\n",
        "            exclude_idx.extend(ecg_indices)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Additional heuristic: components with high kurtosis (muscle artifacts)\n",
        "        # But be more conservative - only top 5% most extreme\n",
        "        try:\n",
        "            if len(ica.mixing_matrix_) > 0:\n",
        "                sources = ica.get_sources(raw_ica).get_data()\n",
        "                # Check for NaN/Inf in sources\n",
        "                if np.any(np.isnan(sources)) or np.any(np.isinf(sources)):\n",
        "                    print(f\"    Warning: NaN/Inf in ICA sources, skipping kurtosis detection...\")\n",
        "                else:\n",
        "                    kurtosis_vals = stats.kurtosis(sources, axis=1)\n",
        "                    # Only mark top 5% most extreme (was 10%)\n",
        "                    high_kurtosis = np.where(kurtosis_vals > np.percentile(kurtosis_vals, 95))[0]\n",
        "                    exclude_idx.extend(high_kurtosis.tolist())\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Remove duplicates\n",
        "        exclude_idx = list(set(exclude_idx))\n",
        "        exclude_idx = [idx for idx in exclude_idx if idx < n_components]\n",
        "        \n",
        "        # Apply ICA if artifacts found\n",
        "        if len(exclude_idx) > 0:\n",
        "            ica.exclude = exclude_idx\n",
        "            raw_cleaned = raw.copy()\n",
        "            ica.apply(raw_cleaned, verbose=False)\n",
        "            \n",
        "            # Final check for NaN/Inf after ICA\n",
        "            data_cleaned = raw_cleaned.get_data()\n",
        "            if np.any(np.isnan(data_cleaned)) or np.any(np.isinf(data_cleaned)):\n",
        "                print(f\"    Warning: NaN/Inf values after ICA, reverting to original...\")\n",
        "                return raw, []\n",
        "            \n",
        "            return raw_cleaned, exclude_idx\n",
        "        else:\n",
        "            return raw, []\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"    Warning: ICA failed for {subject_id}: {str(e)}\")\n",
        "        return raw, []\n",
        "    \n",
        "def zscore_normalize(raw):\n",
        "    \"\"\"Apply Z-score normalization per channel.\"\"\"\n",
        "    raw_normalized = raw.copy()\n",
        "    data = raw_normalized.get_data()\n",
        "    \n",
        "    # Z-score normalization per channel\n",
        "    data_normalized = stats.zscore(data, axis=1)\n",
        "    raw_normalized._data = data_normalized\n",
        "    \n",
        "    return raw_normalized\n",
        "\n",
        "def riemannian_recenter(raw, subject_id):\n",
        "    \"\"\"\n",
        "    Apply Riemannian re-centering if pyriemann is available.\n",
        "    Memory-efficient version using smaller windows with stronger regularization.\n",
        "    \"\"\"\n",
        "    if not HAS_PYRIEMANN:\n",
        "        return raw\n",
        "    \n",
        "    try:\n",
        "        data = raw.get_data().T  # Shape: (n_samples, n_channels)\n",
        "        \n",
        "        # Check for NaN/Inf\n",
        "        if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
        "            print(f\"    Warning: NaN/Inf values detected, skipping Riemannian re-centering...\")\n",
        "            return raw\n",
        "        \n",
        "        # Use smaller windows for memory efficiency\n",
        "        window_length = int(raw.info['sfreq'] * 0.5)  # 0.5 second windows\n",
        "        step_size = window_length // 2\n",
        "        \n",
        "        if data.shape[0] < window_length:\n",
        "            return raw  # Not enough data\n",
        "        \n",
        "        covariances = []\n",
        "        n_windows = min(50, (data.shape[0] - window_length) // step_size + 1)  # Limit windows\n",
        "        \n",
        "        for i in range(n_windows):\n",
        "            start_idx = i * step_size\n",
        "            if start_idx + window_length > data.shape[0]:\n",
        "                break\n",
        "            window_data = data[start_idx:start_idx + window_length, :]\n",
        "            \n",
        "            # Check for NaN/Inf in window\n",
        "            if np.any(np.isnan(window_data)) or np.any(np.isinf(window_data)):\n",
        "                continue  # Skip this window\n",
        "            \n",
        "            cov = np.cov(window_data.T)\n",
        "            \n",
        "            # Stronger regularization to ensure positive definiteness\n",
        "            # Use larger regularization value\n",
        "            reg_value = max(1e-4, np.trace(cov) / cov.shape[0] * 0.01)  # 1% of average diagonal\n",
        "            cov += np.eye(cov.shape[0]) * reg_value\n",
        "            \n",
        "            # Verify positive definiteness\n",
        "            eigenvals = np.linalg.eigvalsh(cov)\n",
        "            if np.any(eigenvals <= 0):\n",
        "                # If still not positive definite, add more regularization\n",
        "                cov += np.eye(cov.shape[0]) * (abs(np.min(eigenvals)) + 1e-4)\n",
        "            \n",
        "            covariances.append(cov)\n",
        "        \n",
        "        if len(covariances) < 3:\n",
        "            print(f\"    Warning: Not enough valid covariance matrices, skipping...\")\n",
        "            return raw\n",
        "        \n",
        "        if len(covariances) > 0:\n",
        "            covariances = np.array(covariances)\n",
        "            \n",
        "            # Additional check: ensure all covariances are positive definite\n",
        "            for i, cov in enumerate(covariances):\n",
        "                eigenvals = np.linalg.eigvalsh(cov)\n",
        "                if np.any(eigenvals <= 0):\n",
        "                    # Add more regularization\n",
        "                    reg_value = abs(np.min(eigenvals)) + 1e-4\n",
        "                    covariances[i] = cov + np.eye(cov.shape[0]) * reg_value\n",
        "            \n",
        "            mean_cov = mean_riemann(covariances)\n",
        "            \n",
        "            # Final check on mean covariance\n",
        "            eigenvals = np.linalg.eigvalsh(mean_cov)\n",
        "            if np.any(eigenvals <= 0):\n",
        "                # Add regularization to mean\n",
        "                reg_value = abs(np.min(eigenvals)) + 1e-4\n",
        "                mean_cov += np.eye(mean_cov.shape[0]) * reg_value\n",
        "                eigenvals = np.linalg.eigvalsh(mean_cov)\n",
        "            \n",
        "            # Whitening\n",
        "            eigenvals = np.maximum(eigenvals, 1e-8)  # Stronger minimum threshold\n",
        "            eigenvecs = np.linalg.eigh(mean_cov)[1]\n",
        "            whitening_matrix = eigenvecs @ np.diag(1.0 / np.sqrt(eigenvals)) @ eigenvecs.T\n",
        "            \n",
        "            data_recentered = (whitening_matrix @ data.T).T\n",
        "            \n",
        "            # Final check for NaN/Inf\n",
        "            if np.any(np.isnan(data_recentered)) or np.any(np.isinf(data_recentered)):\n",
        "                print(f\"    Warning: NaN/Inf after whitening, reverting...\")\n",
        "                return raw\n",
        "            \n",
        "            raw_recentered = raw.copy()\n",
        "            raw_recentered._data = data_recentered.T\n",
        "            \n",
        "            return raw_recentered\n",
        "        else:\n",
        "            return raw\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"    Warning: Riemannian re-centering failed: {str(e)}\")\n",
        "        return raw\n",
        "\n",
        "print(\"✓ Preprocessing functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Main Preprocessing Pipeline\n",
        "\n",
        "Process each subject one at a time, save immediately to avoid memory issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Checking existing preprocessed files...\n",
            "============================================================\n",
            "✓ Found 278 already processed and valid files\n",
            "→ Need to process: 0 files\n",
            "  Total: 278 files\n",
            "\n",
            "============================================================\n",
            "Starting preprocessing pipeline...\n",
            "Output directory: C:\\Users\\Usha Sri\\OneDrive\\Documents\\Parkinson_Project\\preprocessed_data\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Preprocessing Summary\n",
            "============================================================\n",
            "Already processed (skipped): 278\n",
            "Newly processed: 0\n",
            "Failed: 0\n",
            "Total successful: 278/278\n"
          ]
        }
      ],
      "source": [
        "def is_file_valid(file_path):\n",
        "    \"\"\"\n",
        "    Check if a preprocessed file exists and is valid (can be loaded).\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Path to the preprocessed file\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    bool\n",
        "        True if file exists and is valid, False otherwise\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        # Try to load the file to verify it's valid\n",
        "        raw = mne.io.read_raw_fif(file_path, preload=False, verbose=False)\n",
        "        # Check if it has data\n",
        "        if raw.n_times > 0 and len(raw.ch_names) > 0:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except:\n",
        "        # File exists but is corrupted or invalid\n",
        "        return False\n",
        "\n",
        "def process_single_subject(file_info, output_dir, skip_existing=True):\n",
        "    \"\"\"\n",
        "    Process a single subject through the entire pipeline.\n",
        "    Saves the result immediately to avoid memory issues.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_info : dict\n",
        "        Dictionary with subject information\n",
        "    output_dir : str\n",
        "        Directory to save preprocessed data\n",
        "    skip_existing : bool\n",
        "        If True, skip subjects that are already processed\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    success : bool\n",
        "        Whether processing was successful\n",
        "    \"\"\"\n",
        "    subject_id = file_info['subject_id']\n",
        "    output_file = os.path.join(output_dir, f\"{subject_id}_preprocessed.fif\")\n",
        "    \n",
        "    # Check if already processed (verify file is valid, not just exists)\n",
        "    if skip_existing:\n",
        "        if is_file_valid(output_file):\n",
        "            return True  # Return True without printing (will be counted in summary)\n",
        "        elif os.path.exists(output_file):\n",
        "            # File exists but is invalid - remove it and reprocess\n",
        "            print(f\"  {subject_id}: Found invalid file, will reprocess...\")\n",
        "            try:\n",
        "                os.remove(output_file)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    try:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing: {subject_id} ({file_info['group']}) from {file_info['dataset']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Step 1: Load file\n",
        "        print(f\"  Loading file...\")\n",
        "        raw = load_single_file(file_info)\n",
        "        \n",
        "        # Step 2: Standardized preprocessing\n",
        "        print(f\"  Step 1/5: Standardized preprocessing (resample, filter, CAR)...\")\n",
        "        raw = preprocess_raw(raw, subject_id)\n",
        "        \n",
        "        # Step 3: Bad channel detection and interpolation\n",
        "        print(f\"  Step 2/5: Bad channel detection...\")\n",
        "        bads = detect_bad_channels_simple(raw)\n",
        "        if len(bads) > 0:\n",
        "            print(f\"    Found {len(bads)} bad channels: {bads[:10]}{'...' if len(bads) > 10 else ''}\")\n",
        "            raw.info['bads'] = bads\n",
        "            \n",
        "            # Try interpolation with error handling\n",
        "            try:\n",
        "                # Try standard interpolation first\n",
        "                raw.interpolate_bads(reset_bads=True, verbose=False)\n",
        "                interpolation_success = True\n",
        "            except (np.linalg.LinAlgError, ValueError, RuntimeError) as e:\n",
        "                # If interpolation fails (e.g., SVD convergence issues), try alternative methods\n",
        "                print(f\"    Warning: Standard interpolation failed ({type(e).__name__}), trying alternative...\")\n",
        "                interpolation_success = False\n",
        "                \n",
        "                # Try with explicit origin parameter\n",
        "                try:\n",
        "                    # Use a simple origin if head shape fitting fails\n",
        "                    raw.interpolate_bads(reset_bads=True, origin=(0., 0., 0.), verbose=False)\n",
        "                    interpolation_success = True\n",
        "                except:\n",
        "                    # If that also fails, try removing bad channels instead of interpolating\n",
        "                    print(f\"    Warning: Interpolation not possible, removing bad channels instead...\")\n",
        "                    try:\n",
        "                        raw.drop_channels(bads, verbose=False)\n",
        "                        print(f\"    ✓ Bad channels removed (interpolation not possible)\")\n",
        "                        interpolation_success = True\n",
        "                    except Exception as e2:\n",
        "                        print(f\"    Error: Could not remove bad channels: {str(e2)}\")\n",
        "                        # If removal also fails, just mark them as bad but continue\n",
        "                        raw.info['bads'] = bads\n",
        "                        interpolation_success = False\n",
        "            \n",
        "            if interpolation_success:\n",
        "                # Check for NaN/Inf after interpolation\n",
        "                data = raw.get_data()\n",
        "                if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
        "                    print(f\"    Warning: NaN/Inf values after interpolation, cleaning...\")\n",
        "                    # Clean NaN/Inf values\n",
        "                    for i in range(data.shape[0]):\n",
        "                        ch_data = data[i, :]\n",
        "                        if np.any(np.isnan(ch_data)) or np.any(np.isinf(ch_data)):\n",
        "                            valid_data = ch_data[np.isfinite(ch_data)]\n",
        "                            if len(valid_data) > 0:\n",
        "                                replacement = np.median(valid_data)\n",
        "                            else:\n",
        "                                replacement = 0.0\n",
        "                            ch_data[np.isnan(ch_data) | np.isinf(ch_data)] = replacement\n",
        "                            data[i, :] = ch_data\n",
        "                    raw._data = data\n",
        "                \n",
        "                print(f\"    ✓ Bad channels handled\")\n",
        "            else:\n",
        "                print(f\"    Warning: Could not interpolate or remove bad channels, continuing with bad channels marked...\")\n",
        "        else:\n",
        "            print(f\"    No bad channels detected\")\n",
        "        \n",
        "        # Step 4: ICA artifact removal\n",
        "        print(f\"  Step 3/5: ICA artifact removal...\")\n",
        "        raw, excluded_ics = detect_artifacts_ica_simple(raw, subject_id)\n",
        "        if len(excluded_ics) > 0:\n",
        "            print(f\"    ✓ Removed {len(excluded_ics)} artifact components\")\n",
        "        else:\n",
        "            print(f\"    No artifacts detected\")\n",
        "        \n",
        "        # Step 5: Z-score normalization\n",
        "        print(f\"  Step 4/5: Z-score normalization...\")\n",
        "        raw = zscore_normalize(raw)\n",
        "        \n",
        "        # Step 6: Riemannian re-centering (optional)\n",
        "        print(f\"  Step 5/5: Riemannian re-centering...\")\n",
        "        raw = riemannian_recenter(raw, subject_id)\n",
        "        \n",
        "        # Save preprocessed data\n",
        "        print(f\"  Saving preprocessed data to: {output_file}\")\n",
        "        raw.save(output_file, overwrite=True, verbose=False)\n",
        "        \n",
        "        print(f\"  ✓ Successfully processed and saved {subject_id}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        error_type = type(e).__name__\n",
        "        print(f\"  ✗ Error processing {subject_id}: {error_type}: {error_msg}\")\n",
        "        # Store error info for later analysis\n",
        "        if not hasattr(process_single_subject, 'error_log'):\n",
        "            process_single_subject.error_log = []\n",
        "        process_single_subject.error_log.append({\n",
        "            'subject_id': subject_id,\n",
        "            'error_type': error_type,\n",
        "            'error_message': error_msg,\n",
        "            'dataset': file_info.get('dataset', 'unknown')\n",
        "        })\n",
        "        return False\n",
        "\n",
        "# Check existing files before processing\n",
        "print(\"=\" * 60)\n",
        "print(\"Checking existing preprocessed files...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "existing_valid = []\n",
        "existing_invalid = []\n",
        "need_processing = []\n",
        "\n",
        "for file_info in all_file_list:\n",
        "    subject_id = file_info['subject_id']\n",
        "    output_file = os.path.join(OUTPUT_DIR, f\"{subject_id}_preprocessed.fif\")\n",
        "    \n",
        "    if is_file_valid(output_file):\n",
        "        existing_valid.append(subject_id)\n",
        "    elif os.path.exists(output_file):\n",
        "        existing_invalid.append(subject_id)\n",
        "        # Remove invalid file\n",
        "        try:\n",
        "            os.remove(output_file)\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        need_processing.append(subject_id)\n",
        "\n",
        "print(f\"✓ Found {len(existing_valid)} already processed and valid files\")\n",
        "if len(existing_invalid) > 0:\n",
        "    print(f\"⚠ Found {len(existing_invalid)} invalid files (will be reprocessed)\")\n",
        "print(f\"→ Need to process: {len(need_processing)} files\")\n",
        "print(f\"  Total: {len(all_file_list)} files\")\n",
        "\n",
        "# Process all subjects\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Starting preprocessing pipeline...\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize error log\n",
        "process_single_subject.error_log = []\n",
        "\n",
        "# Track progress\n",
        "successful = existing_valid.copy()  # Start with already processed files\n",
        "failed = []\n",
        "failed_info = []  # Store detailed failure information\n",
        "newly_processed = []\n",
        "\n",
        "for i, file_info in enumerate(all_file_list, 1):\n",
        "    subject_id = file_info['subject_id']\n",
        "    \n",
        "    # Only show progress for files that need processing\n",
        "    if subject_id in need_processing:\n",
        "        print(f\"\\n[{len(successful) + len(failed) + 1}/{len(all_file_list)}] Processing: {subject_id}\")\n",
        "    else:\n",
        "        # Skip silently for already processed files\n",
        "        continue\n",
        "    \n",
        "    success = process_single_subject(file_info, OUTPUT_DIR, skip_existing=True)\n",
        "    \n",
        "    if success:\n",
        "        if subject_id not in existing_valid:\n",
        "            newly_processed.append(subject_id)\n",
        "            successful.append(subject_id)\n",
        "        # If already in existing_valid, it's already counted in successful\n",
        "    else:\n",
        "        failed.append(subject_id)\n",
        "        # Get error info if available\n",
        "        if hasattr(process_single_subject, 'error_log') and process_single_subject.error_log:\n",
        "            last_error = process_single_subject.error_log[-1]\n",
        "            if last_error['subject_id'] == subject_id:\n",
        "                failed_info.append(last_error)\n",
        "\n",
        "# Save failed files list and error log\n",
        "failed_file = os.path.join(OUTPUT_DIR, \"failed_subjects.txt\")\n",
        "error_log_file = os.path.join(OUTPUT_DIR, \"error_log.json\")\n",
        "\n",
        "if len(failed) > 0:\n",
        "    # Save list of failed subjects\n",
        "    with open(failed_file, 'w') as f:\n",
        "        for subj in failed:\n",
        "            f.write(f\"{subj}\\n\")\n",
        "    \n",
        "    # Save detailed error log\n",
        "    if failed_info:\n",
        "        import json\n",
        "        with open(error_log_file, 'w') as f:\n",
        "            json.dump(failed_info, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n✓ Saved failed subjects list to: {failed_file}\")\n",
        "    print(f\"✓ Saved error log to: {error_log_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Preprocessing Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Already processed (skipped): {len(existing_valid)}\")\n",
        "print(f\"Newly processed: {len(newly_processed)}\")\n",
        "print(f\"Failed: {len(failed)}\")\n",
        "print(f\"Total successful: {len(successful)}/{len(all_file_list)}\")\n",
        "\n",
        "if len(failed) > 0:\n",
        "    print(f\"\\nFailed subjects ({len(failed)}):\")\n",
        "    \n",
        "    # Group by error type\n",
        "    error_types = {}\n",
        "    for err in failed_info:\n",
        "        err_type = err.get('error_type', 'Unknown')\n",
        "        if err_type not in error_types:\n",
        "            error_types[err_type] = []\n",
        "        error_types[err_type].append(err['subject_id'])\n",
        "    \n",
        "    print(\"\\n  Error breakdown:\")\n",
        "    for err_type, subjects in error_types.items():\n",
        "        print(f\"    {err_type}: {len(subjects)} files\")\n",
        "        if len(subjects) <= 10:\n",
        "            for subj in subjects:\n",
        "                print(f\"      - {subj}\")\n",
        "        else:\n",
        "            for subj in subjects[:5]:\n",
        "                print(f\"      - {subj}\")\n",
        "            print(f\"      ... and {len(subjects) - 5} more\")\n",
        "    \n",
        "    print(f\"\\n  All failed subjects:\")\n",
        "    for subj in failed[:30]:  # Show first 30\n",
        "        print(f\"    - {subj}\")\n",
        "    if len(failed) > 30:\n",
        "        print(f\"    ... and {len(failed) - 30} more\")\n",
        "    \n",
        "    print(f\"\\n  To retry failed files, see the next cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Retry Failed Files (Optional)\n",
        "\n",
        "If you have failed files, you can retry processing them here. This cell will:\n",
        "- Load the list of failed subjects\n",
        "- Attempt to reprocess them\n",
        "- Use the same error handling as the main pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No failed_subjects.txt file found. Run the main pipeline first.\n"
          ]
        }
      ],
      "source": [
        "# Retry failed files\n",
        "RETRY_FAILED = True  # Set to True to retry failed files\n",
        "\n",
        "if RETRY_FAILED:\n",
        "    failed_file = os.path.join(OUTPUT_DIR, \"failed_subjects.txt\")\n",
        "    \n",
        "    if os.path.exists(failed_file):\n",
        "        # Load failed subjects\n",
        "        with open(failed_file, 'r') as f:\n",
        "            failed_subjects = [line.strip() for line in f if line.strip()]\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Retrying {len(failed_subjects)} failed subjects...\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Find file info for failed subjects\n",
        "        failed_file_list = [f for f in all_file_list if f['subject_id'] in failed_subjects]\n",
        "        \n",
        "        if len(failed_file_list) == 0:\n",
        "            print(\"No matching files found in file list.\")\n",
        "        else:\n",
        "            # Initialize error log\n",
        "            process_single_subject.error_log = []\n",
        "            \n",
        "            retry_successful = []\n",
        "            retry_failed = []\n",
        "            \n",
        "            for i, file_info in enumerate(failed_file_list, 1):\n",
        "                subject_id = file_info['subject_id']\n",
        "                print(f\"\\n[{i}/{len(failed_file_list)}] Retrying: {subject_id}\")\n",
        "                \n",
        "                # Remove existing file if it exists (might be corrupted)\n",
        "                output_file = os.path.join(OUTPUT_DIR, f\"{subject_id}_preprocessed.fif\")\n",
        "                if os.path.exists(output_file):\n",
        "                    try:\n",
        "                        os.remove(output_file)\n",
        "                    except:\n",
        "                        pass\n",
        "                \n",
        "                success = process_single_subject(file_info, OUTPUT_DIR, skip_existing=False)\n",
        "                \n",
        "                if success:\n",
        "                    retry_successful.append(subject_id)\n",
        "                else:\n",
        "                    retry_failed.append(subject_id)\n",
        "            \n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"Retry Summary\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Successfully retried: {len(retry_successful)}/{len(failed_file_list)}\")\n",
        "            print(f\"Still failed: {len(retry_failed)}/{len(failed_file_list)}\")\n",
        "            \n",
        "            if len(retry_successful) > 0:\n",
        "                print(f\"\\n✓ Successfully retried:\")\n",
        "                for subj in retry_successful:\n",
        "                    print(f\"  - {subj}\")\n",
        "            \n",
        "            if len(retry_failed) > 0:\n",
        "                print(f\"\\n✗ Still failed:\")\n",
        "                for subj in retry_failed:\n",
        "                    print(f\"  - {subj}\")\n",
        "                \n",
        "                # Update failed subjects file\n",
        "                with open(failed_file, 'w') as f:\n",
        "                    for subj in retry_failed:\n",
        "                        f.write(f\"{subj}\\n\")\n",
        "                print(f\"\\n✓ Updated failed subjects list\")\n",
        "    else:\n",
        "        print(\"No failed_subjects.txt file found. Run the main pipeline first.\")\n",
        "else:\n",
        "    print(\"RETRY_FAILED is set to False. Set it to True to retry failed files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyze Failed Files\n",
        "\n",
        "View detailed error information for failed files to understand what went wrong.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No error log or failed subjects file found. All files processed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Analyze error log\n",
        "error_log_file = os.path.join(OUTPUT_DIR, \"error_log.json\")\n",
        "failed_file = os.path.join(OUTPUT_DIR, \"failed_subjects.txt\")\n",
        "\n",
        "if os.path.exists(error_log_file):\n",
        "    import json\n",
        "    \n",
        "    with open(error_log_file, 'r') as f:\n",
        "        error_log = json.load(f)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"Error Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Total failed files: {len(error_log)}\\n\")\n",
        "    \n",
        "    # Group by error type\n",
        "    error_types = {}\n",
        "    for err in error_log:\n",
        "        err_type = err.get('error_type', 'Unknown')\n",
        "        if err_type not in error_types:\n",
        "            error_types[err_type] = []\n",
        "        error_types[err_type].append(err)\n",
        "    \n",
        "    print(\"Error breakdown by type:\")\n",
        "    for err_type, errors in sorted(error_types.items(), key=lambda x: len(x[1]), reverse=True):\n",
        "        print(f\"\\n  {err_type}: {len(errors)} files\")\n",
        "        \n",
        "        # Show sample error messages\n",
        "        sample_errors = errors[:3]\n",
        "        for err in sample_errors:\n",
        "            msg = err.get('error_message', '')[:100]  # First 100 chars\n",
        "            print(f\"    - {err['subject_id']} ({err.get('dataset', 'unknown')}): {msg}...\")\n",
        "        if len(errors) > 3:\n",
        "            print(f\"    ... and {len(errors) - 3} more with similar errors\")\n",
        "    \n",
        "    # Group by dataset\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"Error breakdown by dataset:\")\n",
        "    dataset_errors = {}\n",
        "    for err in error_log:\n",
        "        dataset = err.get('dataset', 'unknown')\n",
        "        if dataset not in dataset_errors:\n",
        "            dataset_errors[dataset] = []\n",
        "        dataset_errors[dataset].append(err)\n",
        "    \n",
        "    for dataset, errors in sorted(dataset_errors.items(), key=lambda x: len(x[1]), reverse=True):\n",
        "        print(f\"  {dataset}: {len(errors)} files\")\n",
        "    \n",
        "    # Common error patterns\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"Common error patterns:\")\n",
        "    error_messages = {}\n",
        "    for err in error_log:\n",
        "        msg = err.get('error_message', '')\n",
        "        # Extract key part of message (first 50 chars)\n",
        "        key = msg[:50] if len(msg) > 50 else msg\n",
        "        if key not in error_messages:\n",
        "            error_messages[key] = []\n",
        "        error_messages[key].append(err['subject_id'])\n",
        "    \n",
        "    for msg, subjects in sorted(error_messages.items(), key=lambda x: len(x[1]), reverse=True)[:5]:\n",
        "        print(f\"  '{msg}...': {len(subjects)} files\")\n",
        "        if len(subjects) <= 5:\n",
        "            print(f\"    Subjects: {', '.join(subjects)}\")\n",
        "        else:\n",
        "            print(f\"    Subjects: {', '.join(subjects[:5])}... and {len(subjects) - 5} more\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Full error log saved to:\", error_log_file)\n",
        "    print(\"Failed subjects list saved to:\", failed_file)\n",
        "    \n",
        "elif os.path.exists(failed_file):\n",
        "    print(\"Error log not found, but failed subjects list exists.\")\n",
        "    with open(failed_file, 'r') as f:\n",
        "        failed_subjects = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"Failed subjects ({len(failed_subjects)}):\")\n",
        "    for subj in failed_subjects:\n",
        "        print(f\"  - {subj}\")\n",
        "else:\n",
        "    print(\"No error log or failed subjects file found. All files processed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load and Summarize Preprocessed Data\n",
        "\n",
        "Load all preprocessed files and create summary statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Starting subject-level alignment...\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'cleaned_raws' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 109\u001b[0m\n\u001b[0;32m    106\u001b[0m aligned_raws \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    107\u001b[0m aligned_metadata \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m raw, meta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mcleaned_raws\u001b[49m, cleaned_metadata):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Step 1: Z-score normalization\u001b[39;00m\n\u001b[0;32m    112\u001b[0m         raw_normalized \u001b[38;5;241m=\u001b[39m zscore_normalize(raw, meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'cleaned_raws' is not defined"
          ]
        }
      ],
      "source": [
        "def zscore_normalize(raw, subject_id):\n",
        "    \"\"\"\n",
        "    Apply Z-score normalization to each channel independently.\n",
        "    This normalizes each subject's data to have zero mean and unit variance.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw : mne.io.Raw\n",
        "        Artifact-corrected Raw object\n",
        "    subject_id : str\n",
        "        Subject identifier\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    raw_normalized : mne.io.Raw\n",
        "        Z-score normalized Raw object\n",
        "    \"\"\"\n",
        "    print(f\"  Z-score normalizing {subject_id}...\")\n",
        "    \n",
        "    raw_normalized = raw.copy()\n",
        "    data = raw_normalized.get_data()\n",
        "    \n",
        "    # Z-score normalization per channel\n",
        "    # (data - mean) / std\n",
        "    data_normalized = stats.zscore(data, axis=1)\n",
        "    \n",
        "    raw_normalized._data = data_normalized\n",
        "    \n",
        "    print(f\"  ✓ Z-score normalization complete\")\n",
        "    \n",
        "    return raw_normalized\n",
        "\n",
        "def riemannian_recenter(raw, subject_id):\n",
        "    \"\"\"\n",
        "    Apply Riemannian re-centering to align data distributions.\n",
        "    This computes the geometric mean of covariance matrices and whitens the data.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw : mne.io.Raw\n",
        "        Z-score normalized Raw object\n",
        "    subject_id : str\n",
        "        Subject identifier\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    raw_recentered : mne.io.Raw\n",
        "        Riemannian re-centered Raw object\n",
        "    \"\"\"\n",
        "    print(f\"  Riemannian re-centering {subject_id}...\")\n",
        "    \n",
        "    try:\n",
        "        # Extract data\n",
        "        data = raw.get_data().T  # Shape: (n_samples, n_channels)\n",
        "        \n",
        "        # Compute covariance matrices using sliding windows\n",
        "        # Use 1-second windows with 50% overlap\n",
        "        window_length = int(raw.info['sfreq'])  # 1 second\n",
        "        step_size = window_length // 2  # 50% overlap\n",
        "        \n",
        "        covariances = []\n",
        "        for start_idx in range(0, data.shape[0] - window_length + 1, step_size):\n",
        "            window_data = data[start_idx:start_idx + window_length, :]\n",
        "            # Compute sample covariance\n",
        "            cov = np.cov(window_data.T)\n",
        "            # Regularize to ensure positive definiteness\n",
        "            cov += np.eye(cov.shape[0]) * 1e-6\n",
        "            covariances.append(cov)\n",
        "        \n",
        "        if len(covariances) > 0:\n",
        "            covariances = np.array(covariances)\n",
        "            \n",
        "            # Compute geometric mean on Riemannian manifold\n",
        "            mean_cov = mean_riemann(covariances)\n",
        "            \n",
        "            # Whiten the data using the geometric mean\n",
        "            # Compute whitening matrix: W = mean_cov^(-1/2)\n",
        "            eigenvals, eigenvecs = np.linalg.eigh(mean_cov)\n",
        "            eigenvals = np.maximum(eigenvals, 1e-10)  # Avoid numerical issues\n",
        "            whitening_matrix = eigenvecs @ np.diag(1.0 / np.sqrt(eigenvals)) @ eigenvecs.T\n",
        "            \n",
        "            # Apply whitening to all data\n",
        "            data_recentered = (whitening_matrix @ data.T).T\n",
        "            \n",
        "            # Create new Raw object\n",
        "            raw_recentered = raw.copy()\n",
        "            raw_recentered._data = data_recentered.T\n",
        "            \n",
        "            print(f\"  ✓ Riemannian re-centering complete\")\n",
        "        else:\n",
        "            print(f\"  Warning: Not enough data for Riemannian re-centering, skipping...\")\n",
        "            raw_recentered = raw.copy()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Riemannian re-centering failed: {str(e)}\")\n",
        "        print(f\"  Continuing without Riemannian re-centering...\")\n",
        "        raw_recentered = raw.copy()\n",
        "    \n",
        "    return raw_recentered\n",
        "\n",
        "# Apply subject-level alignment\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting subject-level alignment...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "aligned_raws = []\n",
        "aligned_metadata = []\n",
        "\n",
        "for raw, meta in zip(cleaned_raws, cleaned_metadata):\n",
        "    try:\n",
        "        # Step 1: Z-score normalization\n",
        "        raw_normalized = zscore_normalize(raw, meta['subject_id'])\n",
        "        \n",
        "        # Step 2: Riemannian re-centering\n",
        "        raw_aligned = riemannian_recenter(raw_normalized, meta['subject_id'])\n",
        "        \n",
        "        aligned_raws.append(raw_aligned)\n",
        "        aligned_metadata.append(meta)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error in alignment for {meta['subject_id']}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✓ Successfully aligned {len(aligned_raws)}/{len(cleaned_raws)} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verify Channel Consistency\n",
        "\n",
        "Check that all subjects have the same channels (as specified in requirements).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Channel Consistency Check\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'summary_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load channel names from a few files to check consistency\u001b[39;00m\n\u001b[0;32m      7\u001b[0m channel_sets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m sample_subjects \u001b[38;5;241m=\u001b[39m \u001b[43msummary_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject_id \u001b[38;5;129;01min\u001b[39;00m sample_subjects:\n\u001b[0;32m     11\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m summary_df[summary_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m subject_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'summary_df' is not defined"
          ]
        }
      ],
      "source": [
        "# Check channel consistency\n",
        "print(\"=\" * 60)\n",
        "print(\"Channel Consistency Check\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load channel names from a few files to check consistency\n",
        "channel_sets = []\n",
        "sample_subjects = summary_df['subject_id'].head(10).tolist()\n",
        "\n",
        "for subject_id in sample_subjects:\n",
        "    file_path = summary_df[summary_df['subject_id'] == subject_id]['file_path'].iloc[0]\n",
        "    try:\n",
        "        raw = mne.io.read_raw_fif(file_path, preload=False, verbose=False)\n",
        "        channel_sets.append(set(raw.ch_names))\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "if len(channel_sets) > 0:\n",
        "    # Check if all have same channels\n",
        "    unique_channel_sets = [list(s) for s in set(tuple(sorted(s)) for s in channel_sets)]\n",
        "    \n",
        "    if len(unique_channel_sets) == 1:\n",
        "        print(f\"✓ All checked subjects have the same channels: {len(unique_channel_sets[0])} channels\")\n",
        "        print(f\"  Channels: {', '.join(sorted(unique_channel_sets[0])[:10])}...\")\n",
        "        COMMON_CHANNELS = sorted(unique_channel_sets[0])\n",
        "    else:\n",
        "        print(f\"⚠ Warning: Found {len(unique_channel_sets)} different channel configurations\")\n",
        "        for i, ch_set in enumerate(unique_channel_sets):\n",
        "            print(f\"  Configuration {i+1}: {len(ch_set)} channels\")\n",
        "            print(f\"    {', '.join(sorted(ch_set)[:10])}...\")\n",
        "        # Use the most common channel set\n",
        "        from collections import Counter\n",
        "        channel_counts = Counter(tuple(sorted(s)) for s in channel_sets)\n",
        "        most_common = channel_counts.most_common(1)[0][0]\n",
        "        COMMON_CHANNELS = sorted(list(most_common))\n",
        "        print(f\"\\n  Using most common channel set: {len(COMMON_CHANNELS)} channels\")\n",
        "else:\n",
        "    print(\"⚠ Could not verify channel consistency\")\n",
        "    COMMON_CHANNELS = None\n",
        "\n",
        "# Verify sampling rate consistency\n",
        "sampling_rates = summary_df['sampling_rate'].unique()\n",
        "if len(sampling_rates) == 1:\n",
        "    print(f\"\\n✓ All subjects have the same sampling rate: {sampling_rates[0]} Hz\")\n",
        "else:\n",
        "    print(f\"\\n⚠ Warning: Found different sampling rates: {sampling_rates}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Extract Data Arrays for Machine Learning (Optional)\n",
        "\n",
        "Extract data arrays from preprocessed files. This can be memory-intensive, so it's optional.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_data_arrays_memory_efficient(summary_df, output_dir, max_subjects=None):\n",
        "    \"\"\"\n",
        "    Extract data arrays from preprocessed files in a memory-efficient way.\n",
        "    Only loads one subject at a time.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    summary_df : pd.DataFrame\n",
        "        Summary dataframe with file paths\n",
        "    output_dir : str\n",
        "        Output directory for saving arrays\n",
        "    max_subjects : int, optional\n",
        "        Maximum number of subjects to process (for testing)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    X_file : str\n",
        "        Path to saved X array file\n",
        "    y_file : str\n",
        "        Path to saved y array file\n",
        "    subject_ids_file : str\n",
        "        Path to saved subject_ids file\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Extracting data arrays for machine learning...\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Note: This may take a while and use significant memory.\")\n",
        "    print(\"      Consider processing in batches if memory is limited.\\n\")\n",
        "    \n",
        "    # Limit subjects if specified\n",
        "    if max_subjects is not None:\n",
        "        summary_df = summary_df.head(max_subjects)\n",
        "        print(f\"Processing first {max_subjects} subjects only...\\n\")\n",
        "    \n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    subject_ids = []\n",
        "    \n",
        "    for idx, row in summary_df.iterrows():\n",
        "        subject_id = row['subject_id']\n",
        "        file_path = row['file_path']\n",
        "        \n",
        "        try:\n",
        "            print(f\"  Loading {subject_id}... ({idx+1}/{len(summary_df)})\")\n",
        "            raw = mne.io.read_raw_fif(file_path, preload=True, verbose=False)\n",
        "            data = raw.get_data()  # Shape: (n_channels, n_samples)\n",
        "            \n",
        "            X_list.append(data)\n",
        "            \n",
        "            # Encode labels: Control = 0, PD = 1\n",
        "            label = 0 if row['group'] == 'Control' else 1\n",
        "            y_list.append(label)\n",
        "            subject_ids.append(subject_id)\n",
        "            \n",
        "            # Clear memory\n",
        "            del raw, data\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"    ✗ Error loading {subject_id}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    if len(X_list) == 0:\n",
        "        print(\"✗ No data extracted!\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # Convert to arrays\n",
        "    print(f\"\\n  Converting to arrays...\")\n",
        "    X = np.array(X_list)  # Shape: (n_subjects, n_channels, n_samples)\n",
        "    y = np.array(y_list)\n",
        "    \n",
        "    # Save arrays\n",
        "    print(f\"  Saving arrays...\")\n",
        "    X_file = os.path.join(output_dir, \"X_preprocessed.npy\")\n",
        "    y_file = os.path.join(output_dir, \"y_labels.npy\")\n",
        "    subject_ids_file = os.path.join(output_dir, \"subject_ids.npy\")\n",
        "    \n",
        "    np.save(X_file, X)\n",
        "    np.save(y_file, y)\n",
        "    np.save(subject_ids_file, np.array(subject_ids, dtype=object))\n",
        "    \n",
        "    print(f\"\\n✓ Data extraction complete!\")\n",
        "    print(f\"  Data shape: {X.shape} (n_subjects, n_channels, n_samples)\")\n",
        "    print(f\"  Labels shape: {y.shape}\")\n",
        "    print(f\"  Control subjects: {np.sum(y == 0)}\")\n",
        "    print(f\"  PD subjects: {np.sum(y == 1)}\")\n",
        "    print(f\"\\n  Data statistics:\")\n",
        "    print(f\"    Mean: {X.mean():.6f}\")\n",
        "    print(f\"    Std: {X.std():.6f}\")\n",
        "    print(f\"    Min: {X.min():.6f}\")\n",
        "    print(f\"    Max: {X.max():.6f}\")\n",
        "    print(f\"\\n  Files saved:\")\n",
        "    print(f\"    {X_file}\")\n",
        "    print(f\"    {y_file}\")\n",
        "    print(f\"    {subject_ids_file}\")\n",
        "    \n",
        "    # Clear memory\n",
        "    del X, y, X_list, y_list\n",
        "    \n",
        "    return X_file, y_file, subject_ids_file\n",
        "\n",
        "# Uncomment to extract data arrays (memory-intensive!)\n",
        "# X_file, y_file, subject_ids_file = extract_data_arrays_memory_efficient(\n",
        "#     summary_df, OUTPUT_DIR, max_subjects=None  # Set max_subjects for testing\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Summary and Metadata\n",
        "\n",
        "Save the preprocessing summary for future reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'summary_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save summary dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m summary_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessing_summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43msummary_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(summary_csv, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Saved preprocessing summary to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a metadata file with preprocessing parameters\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'summary_df' is not defined"
          ]
        }
      ],
      "source": [
        "# Save summary dataframe\n",
        "summary_csv = os.path.join(OUTPUT_DIR, \"preprocessing_summary.csv\")\n",
        "summary_df.to_csv(summary_csv, index=False)\n",
        "print(f\"✓ Saved preprocessing summary to: {summary_csv}\")\n",
        "\n",
        "# Create a metadata file with preprocessing parameters\n",
        "metadata = {\n",
        "    'preprocessing_parameters': {\n",
        "        'target_sampling_rate_hz': TARGET_SFREQ,\n",
        "        'bandpass_low_hz': LOW_FREQ,\n",
        "        'bandpass_high_hz': HIGH_FREQ,\n",
        "        'ica_n_components': ICA_N_COMPONENTS,\n",
        "        'ica_max_iter': ICA_MAX_ITER,\n",
        "        're_referencing': 'Common Average Reference (CAR)',\n",
        "        'normalization': 'Z-score per channel',\n",
        "        'riemannian_recentering': HAS_PYRIEMANN\n",
        "    },\n",
        "    'total_subjects': len(summary_df),\n",
        "    'by_dataset': summary_df['dataset'].value_counts().to_dict(),\n",
        "    'by_group': summary_df['group'].value_counts().to_dict(),\n",
        "    'output_directory': OUTPUT_DIR\n",
        "}\n",
        "\n",
        "import json\n",
        "metadata_file = os.path.join(OUTPUT_DIR, \"preprocessing_metadata.json\")\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"✓ Saved preprocessing metadata to: {metadata_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Preprocessing Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nAll preprocessed files are saved in: {OUTPUT_DIR}\")\n",
        "print(f\"Each subject has been saved as: {{subject_id}}_preprocessed.fif\")\n",
        "print(f\"\\nTo load a preprocessed file:\")\n",
        "print(f\"  import mne\")\n",
        "print(f\"  raw = mne.io.read_raw_fif('{OUTPUT_DIR}/{{subject_id}}_preprocessed.fif', preload=True)\")\n",
        "print(f\"\\nTo extract data arrays for ML (optional, memory-intensive):\")\n",
        "print(f\"  Run the cell in section 6 above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes and Next Steps\n",
        "\n",
        "### Preprocessing Steps Applied:\n",
        "1. **Resampling**: All data resampled to 250 Hz\n",
        "2. **Filtering**: Band-pass filter applied (0.5 - 50 Hz)\n",
        "3. **Re-referencing**: Common Average Reference (CAR) applied\n",
        "4. **Bad Channel Detection**: Automatic detection and interpolation\n",
        "5. **Artifact Removal**: ICA with correlation-based artifact detection (no ICLabel required)\n",
        "6. **Normalization**: Z-score normalization per channel per subject\n",
        "7. **Riemannian Re-centering**: Applied if pyriemann is available\n",
        "\n",
        "### Memory-Efficient Design:\n",
        "- Each subject is processed and saved individually\n",
        "- No need to load all data into memory at once\n",
        "- Can resume processing if interrupted (skips already processed subjects)\n",
        "- Preprocessed files can be loaded individually for further analysis\n",
        "\n",
        "### Loading Preprocessed Data:\n",
        "```python\n",
        "import mne\n",
        "import os\n",
        "\n",
        "# Load a single subject\n",
        "subject_id = \"sub-001\"\n",
        "file_path = os.path.join(OUTPUT_DIR, f\"{subject_id}_preprocessed.fif\")\n",
        "raw = mne.io.read_raw_fif(file_path, preload=True)\n",
        "\n",
        "# Access the data\n",
        "data = raw.get_data()  # Shape: (n_channels, n_samples)\n",
        "```\n",
        "\n",
        "### For Machine Learning:\n",
        "If you need to extract all data into arrays, use the function in section 6.\n",
        "**Warning**: This is memory-intensive and may not be feasible for large datasets.\n",
        "Consider processing in batches or using the individual .fif files directly.\n",
        "\n",
        "The preprocessed data is now ready for further analysis!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
